{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e2a1e992-6796-4a94-ae63-850a494c94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available and set as device.\n"
     ]
    }
   ],
   "source": [
    "# Import Statements:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device Management:\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available and set as device.\")\n",
    "else:\n",
    "    print(\"MPS is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00c06819-bff3-45fe-9055-075feea18b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161611731"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Training Data:\n",
    "with open('chess-moves.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.split()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc2ce6d6-c5da-49e9-9217-e5d4ab64b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reg7+'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Vocabulary and Move Dicts:\n",
    "moves = set(text)\n",
    "vocab_size = len(moves)\n",
    "\n",
    "itom = {i:m for i,m in enumerate(moves)}\n",
    "mtoi = {m:i for i,m in enumerate(moves)}\n",
    "\n",
    "itom[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "869ff297-8fac-4fd1-a8ee-7ad32038cc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1871, 8233, 9584]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Tokenizers:\n",
    "encode = lambda l: [mtoi[m] for m in l]\n",
    "decode = lambda l: [itom[n] for n in l]\n",
    "\n",
    "encode(['Bd4+', 'Rd4', 'd4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "947ac32b-e3f5-4ad1-b2cc-fd45de9ee4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10242,  6666,  9584, 12089,  3217,  5093,  6189,  7502, 10869,  6300,\n",
       "         6288, 11108,  2861,  3224,  2861, 10693, 10242,  5308,  6654,  6654],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Data Tensor:\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data = data.to(device)\n",
    "\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bf87091-7a4a-4d05-87db-b79a871d1e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1402, -1.3340, -0.8495,  ...,  1.2330, -0.9163, -1.3038],\n",
       "        [-0.2831,  0.2614, -1.6091,  ...,  0.4453,  0.0858, -0.7578],\n",
       "        [-1.7222, -0.4916,  0.1578,  ..., -0.3442, -2.2328, -0.2045],\n",
       "        ...,\n",
       "        [-0.0936, -1.1812,  1.8925,  ..., -0.6833, -1.2142,  0.4356],\n",
       "        [-1.8069, -1.9777,  0.9817,  ..., -0.2323,  0.1621, -0.9676],\n",
       "        [ 0.3115,  0.3212, -0.1303,  ...,  1.9315, -1.1168, -0.5739]],\n",
       "       device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Hyperparameters:\n",
    "vocab_size = 12356\n",
    "n_embd = 384\n",
    "head_size = 16\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "batch_size = 16\n",
    "block_size = 128\n",
    "dropout = 0.2\n",
    "\n",
    "# Single Head of Attention:\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # K,Q,V Matrices:\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Buffer Matrix and Dropout Layer:\n",
    "        self.register_buffer('tril', torch.tril(torch.ones([block_size, block_size])))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Determining Affinities with Weighted Sum:\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Adjusting Embedding With Value Matrix:\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Parralelization of Attention Heads:\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, n_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "\n",
    "        # Projection and Dropout Layers:\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Multi-Layer Perceptron:\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear Layers:\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Self-Attention/MLP Block:\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        # Self-Attention/MLP:\n",
    "        self.sa = MultiHeadedAttention(head_size, n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Layer Normalization:\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    # Residual Blocks:\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# AMP Transformer Model:\n",
    "class ChessGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token and Positional Embedding Tables:\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Block Layers:\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        # Layer Normalization and Unembedding:\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # Embedding:\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Creating Logits after Forward Pass:\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Determining Loss via Cross Entropy\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx):\n",
    "\n",
    "        # Generate New Move:\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, loss = self(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_new = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, idx_new], dim=1)\n",
    "        return idx\n",
    "        \n",
    "# Initializing Model\n",
    "m = ChessGPT()\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "\n",
    "# Creating Optimizer:\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
    "\n",
    "list(m.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec197e4e-7426-48f2-9322-e19c35a75e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e4', 'Rbxa2']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untrained Example:\n",
    "game = ['e4']\n",
    "\n",
    "idx = torch.tensor([encode(game)], dtype=torch.long).to(device)\n",
    "\n",
    "decode(m.generate(idx)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b056d328-94e9-4828-9e39-e0c5748c9250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 128]), torch.Size([16, 128]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batching Data:\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch()\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b0f91276-a359-44ee-ab46-57cfe479ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 3.3490\n",
      "step: 1000 loss: 3.3577\n",
      "step: 2000 loss: 3.2876\n",
      "step: 3000 loss: 3.2888\n",
      "step: 4000 loss: 3.2059\n",
      "step: 5000 loss: 3.2145\n",
      "step: 6000 loss: 3.2807\n",
      "step: 7000 loss: 3.2221\n",
      "step: 8000 loss: 3.1621\n",
      "step: 9000 loss: 3.2097\n",
      "total loss: 3.1522 total time: 2082.9466 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "steps = 10000\n",
    "\n",
    "# Training Loop:\n",
    "for step in range(steps):\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 1000 == 0:\n",
    "        print(f'step: {step} loss: {loss:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "print(f'total loss: {loss:.4f} total time: {total:.4f} seconds')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0c46fda6-483e-4b9c-aff1-63a2a4255451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e5'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = ['e4',]\n",
    "idx = torch.tensor([encode(game)], dtype=torch.long).to(device)\n",
    "\n",
    "decode(m.generate(idx)[0].tolist())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "94694d1f-501b-478f-8fda-3598467fd1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f18297d4684e79937f97289a264078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>White to play. Enter your move above.</b>'), HBox(children=(Text(value='', descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def get_move_history_cleaned(board):\n",
    "    move_history = []\n",
    "    temp_board = chess.Board()\n",
    "    \n",
    "    for move in board.move_stack:\n",
    "        san_move = temp_board.san(move)\n",
    "        cleaned_move = san_move.replace('+', '').replace('#', '').replace('x', '')\n",
    "        move_history.append(cleaned_move)\n",
    "        temp_board.push(move)\n",
    "    \n",
    "    return move_history\n",
    "\n",
    "def clean_san_move(san_move):\n",
    "    cleaned = san_move.replace('+', '').replace('#', '').replace('x', '')\n",
    "    return cleaned\n",
    "\n",
    "def new_move(moves):\n",
    "\n",
    "    previous_moves = get_move_history_cleaned(board)\n",
    "\n",
    "    legal_moves_san = []\n",
    "    for move in board.legal_moves:\n",
    "        san_move = board.san(move)\n",
    "        legal_moves_san.append(san_move)\n",
    "\n",
    "    while True:\n",
    "        idx = torch.tensor([encode(previous_moves)], dtype=torch.long).to(device)\n",
    "        move = decode(m.generate(idx)[0].tolist())[-1]\n",
    "        print(move)\n",
    "        move = clean_san_move(move)\n",
    "        if move in legal_moves_san:\n",
    "            print('correct!')\n",
    "            break\n",
    "        \n",
    "    return move\n",
    "\n",
    "# Initialize chess board\n",
    "board = chess.Board()\n",
    "move_history = []\n",
    "\n",
    "# Create widgets\n",
    "move_input = widgets.Text(\n",
    "    value='',\n",
    "    description='Your move:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Play Move',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "reset_button = widgets.Button(\n",
    "    description='Reset Game',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "status_label = widgets.HTML(value=\"<b>White to play. Enter your move above.</b>\")\n",
    "\n",
    "def display_board():\n",
    "    \"\"\"Display the current board position\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Create SVG of the board\n",
    "        svg = chess.svg.board(board=board, size=400)\n",
    "        \n",
    "        # Display the SVG\n",
    "        display(HTML(f'<div style=\"text-align: center;\">{svg}</div>'))\n",
    "        \n",
    "        # Show move history\n",
    "        if move_history:\n",
    "            history_str = \" \".join([f\"{i//2 + 1}.{move}\" if i % 2 == 0 else move \n",
    "                                  for i, move in enumerate(move_history)])\n",
    "            print(f\"\\nMove history: {history_str}\")\n",
    "\n",
    "def play_move(button):\n",
    "    \"\"\"Handle user move and generate AI response\"\"\"\n",
    "    global move_history\n",
    "    \n",
    "    user_move = move_input.value.strip()\n",
    "    \n",
    "    if not user_move:\n",
    "        status_label.value = \"<b style='color: red;'>Please enter a move!</b>\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Parse and validate user move\n",
    "        move = board.parse_san(user_move)\n",
    "        \n",
    "        # Make the user move\n",
    "        board.push(move)\n",
    "        move_history.append(user_move)\n",
    "        \n",
    "        # Check game state after user move\n",
    "        if board.is_game_over():\n",
    "            display_board()\n",
    "            result = board.result()\n",
    "            status_label.value = f\"<b style='color: blue;'>Game Over! Result: {result}</b>\"\n",
    "            move_input.disabled = True\n",
    "            submit_button.disabled = True\n",
    "            return\n",
    "        \n",
    "        status_label.value = \"<b style='color: orange;'>AI is thinking...</b>\"\n",
    "        display_board()\n",
    "        \n",
    "        # Generate AI move\n",
    "        try:\n",
    "            ai_move_san = new_move(move_history)  # Your transformer function\n",
    "            ai_move = board.parse_san(ai_move_san)\n",
    "            \n",
    "            # Make AI move\n",
    "            board.push(ai_move)\n",
    "            move_history.append(ai_move_san)\n",
    "            \n",
    "            # Check game state after AI move\n",
    "            if board.is_game_over():\n",
    "                display_board()\n",
    "                result = board.result()\n",
    "                status_label.value = f\"<b style='color: blue;'>Game Over! Result: {result}</b>\"\n",
    "                move_input.disabled = True\n",
    "                submit_button.disabled = True\n",
    "            else:\n",
    "                status_label.value = f\"<b>AI played: {ai_move_san}. Your turn!</b>\"\n",
    "                display_board()\n",
    "            \n",
    "        except Exception as e:\n",
    "            status_label.value = f\"<b style='color: red;'>AI error: {str(e)}</b>\"\n",
    "            # Undo user move if AI fails\n",
    "            board.pop()\n",
    "            move_history.pop()\n",
    "            display_board()\n",
    "    \n",
    "    except ValueError as e:\n",
    "        status_label.value = f\"<b style='color: red;'>Invalid move: {str(e)}</b>\"\n",
    "    \n",
    "    # Clear input\n",
    "    move_input.value = \"\"\n",
    "\n",
    "def reset_game(button):\n",
    "    \"\"\"Reset the game to starting position\"\"\"\n",
    "    global move_history\n",
    "    board.reset()\n",
    "    move_history = []\n",
    "    move_input.disabled = False\n",
    "    submit_button.disabled = False\n",
    "    move_input.value = \"\"\n",
    "    status_label.value = \"<b>White to play. Enter your move above.</b>\"\n",
    "    display_board()\n",
    "\n",
    "def on_enter_key(change):\n",
    "    \"\"\"Handle Enter key press in text input\"\"\"\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        # Small delay to allow the value to be processed\n",
    "        import time\n",
    "        time.sleep(0.1)\n",
    "        if move_input.value.strip():\n",
    "            play_move(None)\n",
    "\n",
    "# Connect event handlers\n",
    "submit_button.on_click(play_move)\n",
    "reset_button.on_click(reset_game)\n",
    "\n",
    "# Create the interface\n",
    "interface = widgets.VBox([\n",
    "    status_label,\n",
    "    widgets.HBox([move_input, submit_button, reset_button]),\n",
    "    output_area\n",
    "])\n",
    "\n",
    "# Initial board display\n",
    "display_board()\n",
    "\n",
    "# Display the interface\n",
    "display(interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9078d-3f49-4068-824b-08f9f4ae7ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
